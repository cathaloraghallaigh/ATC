{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3e0f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Group Preview (from file headers only) ===\n",
      "Total proteins: 88 | Grouped (named): 88\n",
      "\n",
      "Group: Linear  (n=59)\n",
      "  - A0A059SVB0_L\n",
      "  - A0A068B0N9_L\n",
      "  - A0A097ZLN9_L\n",
      "  - A0A097ZLP5_L\n",
      "  - A0A097ZLS9_L\n",
      "  - A0A140KFH3_L\n",
      "  - A0A1Q1N939_L\n",
      "  - A0A2R3ZE38_L\n",
      "  - A0A2R3ZE39_L\n",
      "  - A0A345BJ04_L\n",
      "  - A0A3G9EWY9_L\n",
      "  - A0A4Y5QWA6_L\n",
      "  - A0A4Y5QZ62_L\n",
      "  - A0A5B9G8E4_L\n",
      "  - A0A6C0M6B5_L\n",
      "  - A0A7G8EIM4_L\n",
      "  - A0A7L7S5P2_L\n",
      "  - A0A7L7S5U1_L\n",
      "  - A0A7L7T499_L\n",
      "  - A0A7L7TAQ2_L\n",
      "  - ATY48639_L\n",
      "  - B1NA83_L\n",
      "  - B1NA84_L\n",
      "  - B6F137_L\n",
      "  - C0KWV5_L\n",
      "  - C0KWV7_L\n",
      "  - C0KY88_L\n",
      "  - C0PPR1_L\n",
      "  - D4N3A0_L\n",
      "  - D4N3A1_L\n",
      "  - D5SL78_L\n",
      "  - DINCTG000_NP1212847_L\n",
      "  - F2XF93_L\n",
      "  - F2XFA6_L\n",
      "  - F8TWD1_L\n",
      "  - F8TWD2_L\n",
      "  - G5CV39_L\n",
      "  - H6UQ81_L\n",
      "  - H6WBC5_L\n",
      "  - HMOCHR44410_L\n",
      "  - J7HWK5_L\n",
      "  - P0CV94_L\n",
      "  - P0CV95_L\n",
      "  - Q1XBU5_L\n",
      "  - Q2XSC5_L\n",
      "  - Q5SBP3_L\n",
      "  - Q675L2_L\n",
      "  - Q6ZH94_L\n",
      "  - Q84UV0_L\n",
      "  - Q8H2B4_L\n",
      "  - Q96376_L\n",
      "  - Q9SPN0_L\n",
      "  - Q9SPN1_L\n",
      "  - R4I6X2_L\n",
      "  - SLECTG065_NP871_L\n",
      "  - SLECTG127_NP665_L\n",
      "  - SLECTG240_NP1756_L\n",
      "  - U5PZT6_L\n",
      "  - VODCTG000225_NP551212959_L\n",
      "\n",
      "Group: Monocyclic  (n=29)\n",
      "  - 2ONG_C\n",
      "  - A0A0D3ML99_C\n",
      "  - A0A0F6PMD0_C\n",
      "  - A0A1C9J6A7_C\n",
      "  - A0A481YAH7_C\n",
      "  - A0A4Y5QVX4_C\n",
      "  - A0A6P6W6H5_C\n",
      "  - A7IZZ1_C\n",
      "  - B0F4G4_C\n",
      "  - O04806_C\n",
      "  - O22340_C\n",
      "  - Q20HU7_C\n",
      "  - Q2XSC6_C\n",
      "  - Q675L1_C\n",
      "  - Q6F5H2_C\n",
      "  - Q6F5H3_C\n",
      "  - Q8L5K1_C\n",
      "  - Q8L5K3_C\n",
      "  - Q940E7_C\n",
      "  - Q9AXM7_C\n",
      "  - Q9FUW5_C\n",
      "  - Q9M5A4_C\n",
      "  - Q9M7C9_C\n",
      "  - Q9SW76_C\n",
      "  - R4YVJ5_C\n",
      "  - VODCTG000225_NP551212.245_C\n",
      "  - VODCTG000245_NP551212567_C\n",
      "  - VRACHR51716_C\n",
      "  - VRACHR52544_C\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies for the script\n",
    "!pip install --quiet numpy pandas matplotlib logomaker biopython\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import logomaker\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from Bio.Data import IUPACData\n",
    "\n",
    "# =============================================================\n",
    "# 1. Imports and configuration\n",
    "# =============================================================\n",
    "INPUT_FILE = \"2ongcealigned.cif\"  # All proteins should be aligned in a single .cif file\n",
    "PROTEIN_OF_INTEREST = \"2ONG_C\".strip().upper()\n",
    "ERROR_THRESHOLD = 4.0\n",
    "HARDEST_FIRST = True\n",
    "DELTA = \"Î”\"\n",
    "FREQ_CUTOFF = 0.15\n",
    "\n",
    "# =============================================================\n",
    "# 1b. Output directory setup\n",
    "# =============================================================\n",
    "OUTPUT_DIR = \"StructuralAlignmentOut\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Normalized output filenames\n",
    "OUTPUT_ASSIGNMENTS = os.path.join(OUTPUT_DIR, \"residue_assignments.csv\")\n",
    "PAIRWISE_DELTA_OUT = os.path.join(\n",
    "    OUTPUT_DIR,\n",
    "    f\"pairwise_delta_freq_ge_{str(FREQ_CUTOFF).replace('.', 'p')}.csv\"\n",
    ")\n",
    "TOPDELTA_OUT = os.path.join(\n",
    "    OUTPUT_DIR,\n",
    "    f\"topdelta_per_position_freq_ge_{str(FREQ_CUTOFF).replace('.', 'p')}.csv\"\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# 2. Grouping options (dynamic, merged labels allowed)\n",
    "# =============================================================\n",
    "# GROUP_MODE:\n",
    "#   - \"grouped\": use GROUP_REGEX to extract a token from the protein id, then map via GROUP_SPEC\n",
    "#   - \"all\":     treat all proteins as one group \"All\"\n",
    "GROUP_MODE = \"grouped\"  # or \"all\"\n",
    "GROUP_REGEX = r\".*_([A-Za-z]+)$\"   # captures trailing token after underscore\n",
    "\n",
    "# Map suffix tokens to human labels. Multiple tokens may map to the same label.\n",
    "# Example: C and M collapse to \"Monocyclic\"; B and T collapse to \"Bicyclic\"; L -> \"Linear\".\n",
    "GROUP_SPEC = [\n",
    "    (\"L\", \"Linear\"),\n",
    "    (\"C\", \"Monocyclic\"),\n",
    "]\n",
    "\n",
    "# Derived mappings and soft validations\n",
    "REGEX_MAP = {tok: name for tok, name in GROUP_SPEC}\n",
    "ORDERED_LABELS = [name for _, name in GROUP_SPEC]  # may contain duplicates by design\n",
    "\n",
    "# Helper: unique in order\n",
    "_def_seen = set()\n",
    "ORDERED_LABELS_UNIQUE = []\n",
    "for lbl in ORDERED_LABELS:\n",
    "    if lbl not in _def_seen:\n",
    "        ORDERED_LABELS_UNIQUE.append(lbl)\n",
    "        _def_seen.add(lbl)\n",
    "\n",
    "# TARGET_MODE:\n",
    "#   - \"manual\": use MANUAL_RESIDUES below\n",
    "#   - \"all\":    use all residue numbers present in the reference protein\n",
    "TARGET_MODE = \"manual\"  # or \"all\"\n",
    "\n",
    "# Manually specified residues (used when TARGET_MODE = \"manual\")\n",
    "MANUAL_RESIDUES = [\n",
    "    63, 315, 324, 345, 348, 349, 352, 353, 356, 427, 430, 452, 453, 454,\n",
    "    458, 492, 493, 496, 499, 500, 502, 503, 504, 507, 509, 512, 573,\n",
    "    577, 578, 579, 581, 582\n",
    "]\n",
    "\n",
    "# 3-letter to single letter AA mapping\n",
    "AA3_TO_1 = {k.upper(): v for k, v in IUPACData.protein_letters_3to1.items()}\n",
    "AA3_TO_1.update({'SEC': 'U', 'PYL': 'O'})  # Include uncommon amino acids\n",
    "\n",
    "# =============================================================\n",
    "# 3. Plot configuration (fonts + logo colour + chunking readability)\n",
    "#    All size values specified in centimeters (cm)\n",
    "# =============================================================\n",
    "# Fonts\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman', 'Times', 'DejaVu Serif', 'serif']\n",
    "\n",
    "# Sequence-logo colour scheme (logomaker built-ins: 'chemistry', 'hydrophobicity', 'charge', 'monochrome')\n",
    "LOGO_COLOR_SCHEME = 'chemistry'\n",
    "\n",
    "# Readability of sequence logos (units: cm)\n",
    "MAX_POSITIONS_PER_FIG = 40   # alignment positions per logo figure (chunk size)\n",
    "MIN_GLOBAL_FREQ = 0.02       # minimum AA frequency to appear in logo \n",
    "WIDTH_PER_POS_CM = 0.7       # cm added per position to figure width\n",
    "BASE_WIDTH_CM = 5.0          # base figure width (cm)\n",
    "FIG_HEIGHT_CM = 10.0         # figure height (cm)\n",
    "DPI = 400                    # saved figure resolution\n",
    "XTICK_LABEL_ROT = 90         # x-axis tick rotation (degrees)\n",
    "\n",
    "# Conversion helper: centimeters to inches for Matplotlib\n",
    "CM_TO_IN = 1.0 / 2.54\n",
    "\n",
    "# Save or not save PNGs of sequence logos\n",
    "SAVE_LOGO_PNG = False\n",
    "\n",
    "# =============================================================\n",
    "# 3b. Quick group preview (headers only)\n",
    "# =============================================================\n",
    "\n",
    "def _scan_protein_ids(path: str):\n",
    "    prots = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"data_\"):\n",
    "                prots.append(line[len(\"data_\"):].strip().upper())\n",
    "    return prots\n",
    "\n",
    "\n",
    "def _group_label_from_id(pid: str) -> str:\n",
    "    if GROUP_MODE == \"all\":\n",
    "        return \"All\"\n",
    "    m = re.match(GROUP_REGEX, pid)\n",
    "    if m:\n",
    "        token = m.group(1)\n",
    "        return REGEX_MAP.get(token, token)\n",
    "    return \"Unassigned\"\n",
    "\n",
    "# Run the preview\n",
    "_all_ids = _scan_protein_ids(INPUT_FILE)\n",
    "print(\"\\n=== Group Preview (from file headers only) ===\")\n",
    "if not _all_ids:\n",
    "    print(\"No 'data_' blocks found in the input file.\")\n",
    "else:\n",
    "    if GROUP_MODE == \"all\":\n",
    "        print(f\"GROUP_MODE='all' -> single group 'All' (n={len(_all_ids)})\")\n",
    "        for p in sorted(_all_ids):\n",
    "            print(f\"  - {p}\")\n",
    "    else:\n",
    "        _preview_groups = {}\n",
    "        for pid in _all_ids:\n",
    "            g = _group_label_from_id(pid)\n",
    "            _preview_groups.setdefault(g, []).append(pid)\n",
    "\n",
    "        total_named = sum(len(_preview_groups[g]) for g in _preview_groups if g != \"Unassigned\")\n",
    "        print(f\"Total proteins: {len(_all_ids)} | Grouped (named): {total_named}\")\n",
    "        for g in sorted(_preview_groups):\n",
    "            members = sorted(_preview_groups[g])\n",
    "            print(f\"\\nGroup: {g}  (n={len(members)})\")\n",
    "            for m in members:\n",
    "                print(f\"  - {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 4. Function: extract alpha carbons\n",
    "# =============================================================\n",
    "\n",
    "def extract_alpha_carbons(path: str):\n",
    "    \"\"\"Parse CIF/PDB-like alignment file and extract CA coordinates.\"\"\"\n",
    "    out = []\n",
    "    current = None\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"data_\"):\n",
    "                current = line[len(\"data_\"):].strip().upper()\n",
    "                continue\n",
    "            if not current:\n",
    "                continue\n",
    "            if line.startswith(\"ATOM\") and \" CA \" in line:\n",
    "                fields = re.split(r\"\\s+\", line.strip())\n",
    "                try:\n",
    "                    resname_3 = fields[5]\n",
    "                    resnum = int(fields[8])\n",
    "                    x, y, z = float(fields[10]), float(fields[11]), float(fields[12])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                aa1 = AA3_TO_1.get(resname_3.upper(), \"-\")\n",
    "                out.append((current, resnum, aa1, np.array([x, y, z], float)))\n",
    "    return out\n",
    "\n",
    "# =============================================================\n",
    "# 5. Extract and prepare reference data \n",
    "# =============================================================\n",
    "alpha_carbons = extract_alpha_carbons(INPUT_FILE)\n",
    "\n",
    "specified_residues = {rnum: aa for pid, rnum, aa, _ in alpha_carbons if pid == PROTEIN_OF_INTEREST}\n",
    "\n",
    "# Decide the target residue set based on TARGET_MODE\n",
    "if TARGET_MODE == \"all\":\n",
    "    TARGET_POSITIONS = sorted({rnum for pid, rnum, aa, _ in alpha_carbons if pid == PROTEIN_OF_INTEREST})\n",
    "else:\n",
    "    TARGET_POSITIONS = list(MANUAL_RESIDUES)\n",
    "\n",
    "# Collect chosen target CA coords from reference\n",
    "target_coords = [\n",
    "    (rnum, aa, coords)\n",
    "    for pid, rnum, aa, coords in alpha_carbons\n",
    "    if pid == PROTEIN_OF_INTEREST and rnum in TARGET_POSITIONS\n",
    "]\n",
    "\n",
    "ref_resname = {rnum: aa for rnum, aa, _ in target_coords}\n",
    "\n",
    "# =============================================================\n",
    "# 6. Group residues by protein (excluding reference protein)\n",
    "# =============================================================\n",
    "by_protein = defaultdict(list)\n",
    "for pid, rnum, aa, xyz in alpha_carbons:\n",
    "    if pid != PROTEIN_OF_INTEREST:\n",
    "        by_protein[pid].append((rnum, aa, xyz))\n",
    "\n",
    "assign_rows = []\n",
    "\n",
    "# =============================================================\n",
    "# 7. Residue mapping and assignment (one-to-one per protein, with error checking)\n",
    "# =============================================================\n",
    "for pid, residues in by_protein.items():\n",
    "    candidates, stats = {}, {}\n",
    "    for t_rnum, _, t_xyz in target_coords:\n",
    "        pairs = [(float(np.linalg.norm(t_xyz - xyz)), rnum, aa) for rnum, aa, xyz in residues]\n",
    "        pairs.sort(key=lambda x: (x[0], x[1]))\n",
    "        candidates[t_rnum] = pairs\n",
    "        within = [p for p in pairs if p[0] <= ERROR_THRESHOLD]\n",
    "        min_d = pairs[0][0] if pairs else float('inf')\n",
    "        second = pairs[1][0] if len(pairs) > 1 else float('inf')\n",
    "        stats[t_rnum] = {'num_in': len(within), 'min_d': min_d, 'ambiguity': second - min_d}\n",
    "\n",
    "    order = sorted([t[0] for t in target_coords],\n",
    "                   key=(lambda r: (stats[r]['num_in'], stats[r]['ambiguity'], stats[r]['min_d'])) if HARDEST_FIRST\n",
    "                        else (lambda r: stats[r]['min_d']))\n",
    "\n",
    "    owner, assign = {}, {}\n",
    "\n",
    "    # initialize per-target diagnostic row\n",
    "    for t_rnum in [t[0] for t in target_coords]:\n",
    "        best = candidates[t_rnum][0] if candidates[t_rnum] else (float('inf'), '-', '-')\n",
    "        assign[t_rnum] = {\n",
    "            'Target Residue Number': t_rnum,\n",
    "            'Target Residue Name': ref_resname[t_rnum],\n",
    "            'Protein': pid,\n",
    "            'Residue Name': '-',\n",
    "            'Residue Number': '-',\n",
    "            'Distance': best[0],\n",
    "            'Status': 'no_unused_within_threshold',\n",
    "            'Nearest Residue Name': best[2],\n",
    "            'Nearest Residue Number': best[1],\n",
    "            'Nearest Distance': best[0],\n",
    "        }\n",
    "\n",
    "    def set_assign(t_num, rnum, aa, dist):\n",
    "        owner[rnum] = (t_num, dist)\n",
    "        assign[t_num] = {\n",
    "            'Target Residue Number': t_num,\n",
    "            'Target Residue Name': ref_resname[t_num],\n",
    "            'Protein': pid,\n",
    "            'Residue Name': aa,\n",
    "            'Residue Number': rnum,\n",
    "            'Distance': dist,\n",
    "            'Status': 'assigned',\n",
    "        }\n",
    "\n",
    "    def reassign_displaced(t_num):\n",
    "        for d2, r2, aa2 in candidates[t_num]:\n",
    "            if d2 <= ERROR_THRESHOLD and r2 not in owner:\n",
    "                set_assign(t_num, r2, aa2, d2)\n",
    "                return True\n",
    "        assign[t_num]['Status'] = 'lost_swap_no_alt'\n",
    "        return False\n",
    "\n",
    "    for t_rnum in order:\n",
    "        for d, rnum, aa in candidates[t_rnum]:\n",
    "            if d > ERROR_THRESHOLD:\n",
    "                break\n",
    "            cur = owner.get(rnum)\n",
    "            if cur is None:\n",
    "                set_assign(t_rnum, rnum, aa, d)\n",
    "                break\n",
    "            prev_t, prev_d = cur\n",
    "            if d < prev_d:\n",
    "                set_assign(t_rnum, rnum, aa, d)\n",
    "                if not reassign_displaced(prev_t):\n",
    "                    pass\n",
    "                break\n",
    "\n",
    "    for t_rnum in [t[0] for t in target_coords]:\n",
    "        assign_rows.append(assign[t_rnum])\n",
    "\n",
    "# =============================================================\n",
    "# 8. Save and validate assignments\n",
    "# =============================================================\n",
    "assign_df = pd.DataFrame(assign_rows)\n",
    "assign_df.to_csv(OUTPUT_ASSIGNMENTS, index=False)\n",
    "print(f\"Saved residue assignments to: {OUTPUT_ASSIGNMENTS}\")\n",
    "\n",
    "mask_assigned = assign_df[\"Status\"] == \"assigned\"\n",
    "reuse = (\n",
    "    assign_df[mask_assigned]\n",
    "    .groupby([\"Protein\", \"Residue Number\"])['Target Residue Number']\n",
    "    .nunique()\n",
    ")\n",
    "reuse = reuse[reuse > 1]\n",
    "if not reuse.empty:\n",
    "    print(\"WARNING: some residues were reused across targets:\")\n",
    "    print(reuse)\n",
    "\n",
    "# =============================================================\n",
    "# 9. Grouping \n",
    "# =============================================================\n",
    "if GROUP_MODE == 'all':\n",
    "    assign_df['Group'] = 'All'\n",
    "else:\n",
    "    def _extract_group(protein_id: str) -> str:\n",
    "        m = re.match(GROUP_REGEX, protein_id)\n",
    "        if m:\n",
    "            token = m.group(1)\n",
    "            return REGEX_MAP.get(token, token)\n",
    "        return 'Unassigned'\n",
    "    assign_df['Group'] = assign_df['Protein'].apply(_extract_group)\n",
    "\n",
    "print(\"\\n=== Group Summary ===\")\n",
    "present_groups_raw = [g for g in sorted(assign_df['Group'].dropna().unique().tolist()) if g != 'Unassigned']\n",
    "if GROUP_MODE == 'all':\n",
    "    print(\"Running in 'all' mode: single group 'All'.\")\n",
    "else:\n",
    "    if not present_groups_raw:\n",
    "        print(\"No valid groups found (all proteins Unassigned).\")\n",
    "    else:\n",
    "        # keep order based on ORDERED_LABELS_UNIQUE, then append any others\n",
    "        seen = set()\n",
    "        present_groups = []\n",
    "        for lbl in ORDERED_LABELS_UNIQUE:\n",
    "            if lbl in present_groups_raw and lbl not in seen:\n",
    "                present_groups.append(lbl)\n",
    "                seen.add(lbl)\n",
    "        for lbl in present_groups_raw:\n",
    "            if lbl not in seen:\n",
    "                present_groups.append(lbl)\n",
    "                seen.add(lbl)\n",
    "\n",
    "        print(f\"Total groups detected: {len(present_groups)}\")\n",
    "        for g in present_groups:\n",
    "            members = sorted(assign_df.loc[assign_df['Group'] == g, 'Protein'].unique().tolist())\n",
    "            print(f\"\\nGroup: {g}  (n={len(members)})\")\n",
    "            for m in members:\n",
    "                print(f\"  - {m}\")\n",
    "\n",
    "# =============================================================\n",
    "# 10. Frequency computation \n",
    "# =============================================================\n",
    "freq_rows = []\n",
    "use_groups = (present_groups if GROUP_MODE == 'grouped' else ['All'])\n",
    "\n",
    "for group in use_groups:\n",
    "    gdf = assign_df[assign_df['Group'] == group] if GROUP_MODE == 'grouped' else assign_df.copy()\n",
    "    for pos in TARGET_POSITIONS:\n",
    "        sdf = gdf[gdf['Target Residue Number'] == pos]\n",
    "        counts = sdf['Residue Name'].value_counts(normalize=True)\n",
    "        for aa, fr in counts.items():\n",
    "            freq_rows.append({\n",
    "                \"Group\": group,\n",
    "                \"Residue Position\": pos,\n",
    "                \"Residue Name\": aa,\n",
    "                \"Frequency\": fr,\n",
    "            })\n",
    "\n",
    "freq_df = pd.DataFrame(freq_rows)\n",
    "if freq_df.empty:\n",
    "    raise SystemExit(\"No frequency data computed. Check grouping or targets and inputs.\")\n",
    "\n",
    "# Reindex positions for plotting \n",
    "unique_positions = freq_df[\"Residue Position\"].unique()\n",
    "position_mapping = {pos: idx for idx, pos in enumerate(sorted(unique_positions))}\n",
    "reverse_position_mapping = {v: k for k, v in position_mapping.items()}\n",
    "freq_df[\"Reindexed Position\"] = freq_df[\"Residue Position\"].map(position_mapping)\n",
    "\n",
    "# =============================================================\n",
    "# 11. Î”-frequency summary per position\n",
    "# =============================================================\n",
    "if GROUP_MODE == 'grouped':\n",
    "    if len(use_groups) < 2:\n",
    "        print(\"\\nÎ”-frequency summary skipped, need at least 2 groups.\")\n",
    "    else:\n",
    "        pivot = freq_df.pivot_table(\n",
    "            index=[\"Residue Position\", \"Residue Name\"],\n",
    "            columns=\"Group\",\n",
    "            values=\"Frequency\",\n",
    "            fill_value=0.0,\n",
    "        ).reset_index()\n",
    "\n",
    "        # ensure all present groups exist as columns\n",
    "        for gcol in use_groups:\n",
    "            if gcol not in pivot.columns:\n",
    "                pivot[gcol] = 0.0\n",
    "\n",
    "        pair_rows = []\n",
    "        for g1, g2 in combinations(use_groups, 2):\n",
    "            sub = pivot[[\"Residue Position\", \"Residue Name\", g1, g2]].copy()\n",
    "            sub[\"delta_mag\"] = (sub[g1] - sub[g2]).abs()\n",
    "            # Keep only letters passing frequency cutoff in at least one group\n",
    "            mask = (sub[g1] >= FREQ_CUTOFF) | (sub[g2] >= FREQ_CUTOFF)\n",
    "            sub = sub[mask]\n",
    "            sub.insert(0, \"Group 1\", g1)\n",
    "            sub.insert(1, \"Group 2\", g2)\n",
    "            pair_rows.append(sub)\n",
    "\n",
    "        if pair_rows:\n",
    "            pairwise_delta_df = pd.concat(pair_rows, ignore_index=True)\n",
    "            pairwise_delta_df.to_csv(PAIRWISE_DELTA_OUT, index=False)\n",
    "            print(f\"\\nPairwise Î”-frequency table saved to {PAIRWISE_DELTA_OUT}\")\n",
    "\n",
    "            # Per-position top delta across all pairs and residues\n",
    "            topdelta = (\n",
    "                pairwise_delta_df.sort_values([\"Residue Position\", \"delta_mag\"], ascending=[True, False])\n",
    "                .groupby(\"Residue Position\", as_index=False)\n",
    "                .first()\n",
    "            )\n",
    "            topdelta.to_csv(TOPDELTA_OUT, index=False)\n",
    "            print(f\"Top-Î” per position table saved to {TOPDELTA_OUT}\")\n",
    "            try:\n",
    "                from IPython.display import display\n",
    "                display(topdelta)\n",
    "            except Exception:\n",
    "                print(topdelta.to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nNo Î” rows passed the frequency cutoff. Try lowering FREQ_CUTOFF.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Î”-frequency summary because GROUP_MODE='all'.\")\n",
    "\n",
    "# =============================================================\n",
    "# 12. Sequence logos plotting\n",
    "# =============================================================\n",
    "valid_letters = set(AA3_TO_1.values())\n",
    "_groups_for_plot = use_groups\n",
    "\n",
    "for group in _groups_for_plot:\n",
    "    gdf = freq_df[freq_df['Group'] == group].copy()\n",
    "    gp = gdf.pivot_table(index='Reindexed Position', columns='Residue Name', values='Frequency', fill_value=0.0)\n",
    "    aa_cols = [c for c in gp.columns if isinstance(c, str) and len(c) == 1 and c in valid_letters]\n",
    "    gp = gp[sorted(aa_cols)]\n",
    "    if gp.empty:\n",
    "        print(f\"[skip] No valid AA columns for group '{group}'.\")\n",
    "        continue\n",
    "\n",
    "    # Prune globally-rare letters for readability\n",
    "    global_max = gp.max(axis=0)\n",
    "    keep_cols = [c for c in gp.columns if global_max[c] >= MIN_GLOBAL_FREQ]\n",
    "    gp = gp[keep_cols] if keep_cols else gp\n",
    "\n",
    "    gp = gp.sort_index()\n",
    "    all_pos = gp.index.to_list()\n",
    "    chunks = [all_pos[i:i+MAX_POSITIONS_PER_FIG] for i in range(0, len(all_pos), MAX_POSITIONS_PER_FIG)]\n",
    "\n",
    "    for ci, chunk in enumerate(chunks, start=1):\n",
    "        window = gp.loc[chunk]\n",
    "        xticklabels = [reverse_position_mapping[i] for i in window.index]\n",
    "        window_seq = window.reset_index(drop=True)  # align glyphs to 0..n-1 for plotting only\n",
    "\n",
    "        # Compute figure size in cm, convert to inches for Matplotlib\n",
    "        fig_width_cm = max(BASE_WIDTH_CM, BASE_WIDTH_CM + WIDTH_PER_POS_CM * len(xticklabels))\n",
    "        fig_height_cm = FIG_HEIGHT_CM\n",
    "        fig, ax = plt.subplots(figsize=(fig_width_cm * CM_TO_IN, fig_height_cm * CM_TO_IN))\n",
    "\n",
    "        _ = logomaker.Logo(window_seq, ax=ax, color_scheme=LOGO_COLOR_SCHEME)\n",
    "\n",
    "        ax.set_xlim(-0.5, len(xticklabels) - 0.5)\n",
    "        ax.set_xlabel('Residue Position', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "        ax.set_yticks(np.arange(0, 1.01, 0.2))\n",
    "        ax.set_yticklabels([f\"{t:.1f}\" for t in np.arange(0, 1.01, 0.2)], fontsize=11, fontweight='bold')\n",
    "        ax.set_xticks(range(len(xticklabels)))\n",
    "        ax.set_xticklabels(xticklabels, rotation=XTICK_LABEL_ROT, fontsize=10, fontweight='bold')\n",
    "        for s in ax.spines.values():\n",
    "            s.set_linewidth(1.2)\n",
    "\n",
    "        # Title shows the group name (e.g., \"Linear\", \"Monocyclic\", \"Bicyclic\")\n",
    "        ax.set_title(group, fontsize=16, pad=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if SAVE_LOGO_PNG:\n",
    "            safe_group = re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", group).lower()\n",
    "            first_label, last_label = xticklabels[0], xticklabels[-1]\n",
    "            out_png = os.path.join(\n",
    "                OUTPUT_DIR,\n",
    "                f\"{safe_group}_logo_chunk{ci}_pos{first_label}-{last_label}.png\"\n",
    "            )\n",
    "            plt.savefig(out_png, dpi=DPI, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "# =============================================================\n",
    "# 13. Per-protein summary output\n",
    "# =============================================================\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "except Exception:\n",
    "    _display = None\n",
    "\n",
    "_df = assign_df.copy()\n",
    "_df['Target Residue Number'] = pd.to_numeric(_df['Target Residue Number'], errors='coerce')\n",
    "_df['Residue Number'] = pd.to_numeric(_df['Residue Number'].where(_df['Residue Number'] != '-', np.nan), errors='coerce')\n",
    "\n",
    "for pid in sorted(_df['Protein'].unique()):\n",
    "    sub = _df[_df['Protein'] == pid].copy()\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Protein: {pid} (rows={len(sub)})\")\n",
    "    cols = ['Target Residue Number', 'Target Residue Name', 'Residue Name', 'Residue Number', 'Distance', 'Status']\n",
    "    out = sub.sort_values(['Target Residue Number'])[cols]\n",
    "    if _display:\n",
    "        _display(out)\n",
    "    else:\n",
    "        print(out.to_string(index=False))\n",
    "\n",
    "# =============================================================\n",
    "# 14. Completion message\n",
    "# =============================================================\n",
    "print(\"\\nScript completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
