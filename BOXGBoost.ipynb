{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931a642",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --quiet pandas matplotlib scikit-optimize xgboost\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "from xgboost import XGBClassifier\n",
    "import heapq\n",
    "import csv\n",
    "\n",
    "# Load data\n",
    "file_path = 'TestTrainSet.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop non-feature columns if present\n",
    "data_clean = data.drop(columns=['Protein'], errors='ignore')\n",
    "\n",
    "# Target column\n",
    "y = data_clean['Cyclical']\n",
    "\n",
    "# Feature groups (strings only)\n",
    "feature_groups = [\n",
    "    ['Exclude', 'Glycine - G'],\n",
    "    ['Exclude', 'Alanine - A'],\n",
    "    ['Exclude', 'Leucine - L'],\n",
    "    ['Exclude', 'Methionine - M'],\n",
    "    ['Exclude', 'Phenylalanine - F'],\n",
    "    ['Exclude', 'Tryptophan - W'],\n",
    "    ['Exclude', 'Lysine - K'],\n",
    "    ['Exclude', 'Glutamine - Q'],\n",
    "    ['Exclude', 'Glutamic Acid - E'],\n",
    "    ['Exclude', 'Serine - S'],\n",
    "    ['Exclude', 'Proline - P'],\n",
    "    ['Exclude', 'Valine - V'],\n",
    "    ['Exclude', 'Isoleucine - I'],\n",
    "    ['Exclude', 'Cysteine - C'],\n",
    "    ['Exclude', 'Tyrosine - Y'],\n",
    "    ['Exclude', 'Histidine - H'],\n",
    "    ['Exclude', 'Arginine - R'],\n",
    "    ['Exclude', 'Asparagine - N'],\n",
    "    ['Exclude', 'Aspartic Acid - D'],\n",
    "    ['Exclude', 'Threonine - T'],\n",
    "    ['Exclude', 'Charged | Uncharged', 'Charge Score', 'Positive charge | Negative charge', 'Pos-neg charge ratio'],\n",
    "    ['Exclude', 'Polar | Non-polar', 'Polarity Score', 'Hydrophobicity Score', 'Local hydrophobic density Score',\n",
    "     'Number of apolar alpha sphere', 'Proportion of apolar alpha sphere', 'Hydrophobic', 'Mean alpha-sphere solvent acc'],\n",
    "    ['Exclude', 'Drug Score'],\n",
    "    ['Exclude', 'Small', 'Number of alpha spheres | Mean alpha-sphere radius', 'Pocket volume (Monte Carlo)', 'Pocket volume (Convex hull)'],\n",
    "    ['Exclude', 'Amino Acid based volume Score'],\n",
    "    ['Exclude', 'Aromatic'],\n",
    "    ['Exclude', 'Number of residues']\n",
    "]\n",
    "\n",
    "# Tuple expansion map\n",
    "string_to_tuple = {\n",
    "    'Charged | Uncharged': ('Charged', 'Uncharged'),\n",
    "    'Positive charge | Negative charge': ('Positive charge', 'Negative charge'),\n",
    "    'Polar | Non-polar': ('Polar', 'Non-polar'),\n",
    "    'Number of alpha spheres | Mean alpha-sphere radius': ('Number of alpha spheres', 'Mean alpha-sphere radius')\n",
    "}\n",
    "\n",
    "# Precompute sample weights\n",
    "class_freq = y.value_counts(normalize=True).to_dict()\n",
    "weight_map = {k: 1 / v for k, v in class_freq.items()}\n",
    "sample_weights = y.map(weight_map)\n",
    "\n",
    "# Leave-one-out splitter\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Expand a chosen combination to explicit feature names\n",
    "\n",
    "def expand_features(feature_combination):\n",
    "    selected = []\n",
    "    for subgroup in feature_combination:\n",
    "        if subgroup == 'Exclude':\n",
    "            continue\n",
    "        if subgroup in string_to_tuple:\n",
    "            selected.extend(string_to_tuple[subgroup])\n",
    "        else:\n",
    "            selected.append(subgroup)\n",
    "    return selected\n",
    "\n",
    "# Build the XGB model\n",
    "\n",
    "def build_xgb():\n",
    "    return XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        verbosity=0,\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "# LOO-CV accuracy for a feature set\n",
    "\n",
    "def loo_cv(X, y, sw):\n",
    "    correct = 0\n",
    "    for tr_idx, te_idx in loo.split(X):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "        sw_tr = sw.iloc[tr_idx]\n",
    "        model = build_xgb()\n",
    "        model.fit(X_tr, y_tr, sample_weight=sw_tr, verbose=False)\n",
    "        y_pred = model.predict(X_te)\n",
    "        correct += int(y_pred[0] == y_te.values[0])\n",
    "    return correct / len(y)\n",
    "\n",
    "# Search space: one categorical per group\n",
    "search_space = [Categorical(group, name=f'group_{i}') for i, group in enumerate(feature_groups)]\n",
    "\n",
    "# Tracking variables\n",
    "all_results = []              # (accuracy, features)\n",
    "best_so_far = []              # running best accuracy\n",
    "iteration_counter = 0         # iteration count\n",
    "val_scores = []               # accuracy per iteration for early stopping\n",
    "\n",
    "# Prepare CSV log\n",
    "with open('feature_combination_log.csv', mode='w', newline='') as f:\n",
    "    csv.writer(f).writerow(['Iteration', 'Accuracy', 'Selected Features'])\n",
    "\n",
    "# Objective for gp_minimize\n",
    "\n",
    "def objective(choice_vector):\n",
    "    global iteration_counter\n",
    "    iteration_counter += 1\n",
    "    selected_features = expand_features(choice_vector)\n",
    "\n",
    "    if not selected_features:\n",
    "        print(f\"Iteration {iteration_counter}: no features selected.\")\n",
    "        acc = 0.0\n",
    "    else:\n",
    "        X_new = data_clean[selected_features]\n",
    "        acc = loo_cv(X_new, y, sample_weights)\n",
    "\n",
    "    all_results.append((acc, selected_features))\n",
    "    with open('feature_combination_log.csv', mode='a', newline='') as f:\n",
    "        csv.writer(f).writerow([iteration_counter, acc, selected_features])\n",
    "\n",
    "    if best_so_far:\n",
    "        best_so_far.append(max(best_so_far[-1], acc))\n",
    "    else:\n",
    "        best_so_far.append(acc)\n",
    "\n",
    "    print(f\"Iteration {iteration_counter}: accuracy={acc:.4f}, features={selected_features}\")\n",
    "    return -acc\n",
    "\n",
    "# Early stopping callback\n",
    "patience = 100\n",
    "threshold = 1e-4\n",
    "\n",
    "def early_stop(res):\n",
    "    # res.fun is negative accuracy\n",
    "    val_scores.append(-res.fun)\n",
    "    if len(val_scores) <= patience:\n",
    "        return False\n",
    "    # Check the max improvement in the last `patience` steps\n",
    "    recent = val_scores[-patience-1:]\n",
    "    improvements = [abs(recent[i+1] - recent[i]) for i in range(len(recent)-1)]\n",
    "    if max(improvements) < threshold:\n",
    "        print(f\"Early stopping at iteration {len(val_scores)}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Run Bayesian optimisation\n",
    "gp_result = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=search_space,\n",
    "    n_calls=500,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    acq_func='LCB',\n",
    "    kappa=10.0,\n",
    "    callback=[early_stop]\n",
    ")\n",
    "\n",
    "# Extract best combination and top 10\n",
    "best_combo = gp_result.x\n",
    "best_features = expand_features(best_combo)\n",
    "\n",
    "top10 = heapq.nlargest(10, all_results, key=lambda x: x[0])\n",
    "print(\"\\nTop 10 feature combinations:\")\n",
    "for i, (acc, feats) in enumerate(top10, 1):\n",
    "    print(f\"{i}. accuracy={acc:.4f}, features={feats}\")\n",
    "\n",
    "print(\"\\nBest feature combination:\", best_features)\n",
    "print(\"Best accuracy:\", -gp_result.fun)\n",
    "\n",
    "# Convergence plot\n",
    "plot_convergence(gp_result)\n",
    "plt.show()\n",
    "\n",
    "# Best-so-far accuracy per iteration\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(best_so_far)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Best accuracy so far')\n",
    "plt.title('Best accuracy over iterations')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
